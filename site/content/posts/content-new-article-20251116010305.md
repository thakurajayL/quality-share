---
title: Metaâ€™s Generative Ads Model (GEM): The Central Brain Accelerating Ads Recommendation AI Innovation
published_date: 2025-11-10T17:00:01+00:00
link: https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/
summary: Meta has developed the Generative Ads Recommendation Model (GEM) to improve ad performance and advertiser ROI by enhancing ad relevance. GEM's architecture allows for efficient scaling with a large number of parameters and has shown significant increases in ad conversions on Instagram and Facebook. It is the largest foundation model for recommendation systems in the industry, trained at the scale of large language models. GEM overcomes challenges through scalable architecture, post-training techniques, and enhanced training infrastructure. Representation learning and parameter sharing are key techniques used in training the GEM model for ad click prediction. The future of ads recommendation systems will focus on personalization and understanding user intent, with GEM playing a central role.
tags:
- AI
- Meta
- Generative Ads Recommendation Model
- GEM
- ad performance
- advertiser ROI
- architecture
- scalability
- parameters
- predictions
- post-training techniques
- training scalability
- GPUs
- LLM-scale ads foundation model
- conversions
- Instagram
- Facebook
- RecSys
- large language models
- knowledge transfer
- personalized ads
- user preferences
- model architecture
- data
- compute
- model scaling
- advanced architecture
- knowledge transfer techniques
- training infrastructure
- feature space
- user-ad interactions
- data processing
- training efficiency
- ad content
- user engagement
- sequence features
- non-sequence features
- attention mechanisms
- feature interaction modeling
- user behavior sequences
- cross-feature learning
- multi-domain learning
- domain-specific optimization
- post-training techniques
- knowledge distillation
- representation learning
- parameter sharing
- training stack
- effective training FLOPS
- GPU utilization
- distributed training
- system-level optimizations
- GPU throughput
- memory compression
- training overhead
- job startup time
- GPU efficiency
- model lifecycle
- future of ads recommendation systems.
content_type: ContentType.BLOG_POST
---

